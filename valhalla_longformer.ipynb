{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from helpers_function import *\n",
    "from data import get_data\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
    "# Model = AutoModelForQuestionAnswering.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\", return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls, company_names, segment = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'valhalla'\n",
    "merge_data(model, company_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "# questions = ['what is the name of this company?', 'when was this company established?', 'what is the company ISO certification?', 'what is the total number of employees?', 'what is the growth rate of this company?', 'what is the revenue of this company?', 'where is the global HQ location of this company?', 'on what field this company works?', 'what is the vision of this company?', 'what are the past awards that has been awarded to this company?', 'Describe about this company.', 'what is the name of the parent company?', 'what is the company external ICT spend?', 'what is the company R&D spend?', 'what is the company R&D focus?', 'what is the marketing budget of this company?', 'what is the number of patents of this company?']\n",
    "questions = ['what is the name of the parent company?', 'when was this company established?', 'Describe about this company.', 'where is the global Headquater of this company?', 'where is the geographical presence of this company?', 'who is the owner of this company?', 'what are the core business units of this company?', 'what is the total number of subsidiaries?', 'what is the total number of employees?', 'what is the total revenue of this company?', 'what is the growth rate of this company?', 'what is the operating profits of this company?', 'Revenue by Region of this company?', 'what are the venture investments?', 'External ICT spend?', 'total marketing budget?', 'what is the R&D focus of this company?', 'what is the R&D spend of this company?', 'total number of patents of this company?', 'what is the vision of this company?', 'what are the strategic priorities?', 'who are the partners of this company?', 'what is the ISO certification of this company?', 'what are the past awards that has been awarded to this company?']\n",
    "print(len(questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ans_path = \"D:/Machine Learning/scrapper and extraction/valhalla_answers\"\n",
    "\n",
    "# if not os.path.exists(ans_path):\n",
    "#     os.mkdir(ans_path)\n",
    "\n",
    "# final_answer = []\n",
    "\n",
    "# for i, company in tqdm(enumerate(company_names)):\n",
    "#     answers = []\n",
    "#     try:\n",
    "#         for j in range(5):\n",
    "#             path = f\"D:/Machine Learning/scrapper and extraction/raw_text/{company}/my_file_{j}.txt\"\n",
    "#             text = get_text(path)\n",
    "\n",
    "#             if text == \"\":\n",
    "#                 continue\n",
    "\n",
    "#             ans = []\n",
    "\n",
    "#             for ques in questions:\n",
    "#                 text = text\n",
    "#                 question = ques\n",
    "#                 encoding = tokenizer(question, text, return_tensors=\"pt\")\n",
    "#                 input_ids = encoding[\"input_ids\"]\n",
    "\n",
    "#                 # default is local attention everywhere\n",
    "#                 # the forward method will automatically set global attention on question tokens\n",
    "#                 attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "#                 start_scores, end_scores = Model(input_ids, attention_mask=attention_mask)\n",
    "#                 all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "\n",
    "#                 answer_tokens = all_tokens[torch.argmax(start_scores) :torch.argmax(end_scores)+1]\n",
    "#                 answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
    "\n",
    "#                 ans.append(answer)\n",
    "            \n",
    "#             answers.append(ans)\n",
    "    \n",
    "#     except:\n",
    "#         continue\n",
    "\n",
    "#     final_answer.append(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Some weights of the model checkpoint at valhalla/longformer-base-4096-finetuned-squadv1 were not used when initializing LongformerForQuestionAnswering: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
      "- This IS expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "1it [06:06, 366.51s/it]Some weights of the model checkpoint at valhalla/longformer-base-4096-finetuned-squadv1 were not used when initializing LongformerForQuestionAnswering: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
      "- This IS expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2it [14:41, 453.78s/it]Some weights of the model checkpoint at valhalla/longformer-base-4096-finetuned-squadv1 were not used when initializing LongformerForQuestionAnswering: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
      "- This IS expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "3it [23:14, 480.97s/it]Some weights of the model checkpoint at valhalla/longformer-base-4096-finetuned-squadv1 were not used when initializing LongformerForQuestionAnswering: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
      "- This IS expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "4it [32:59, 521.88s/it]Some weights of the model checkpoint at valhalla/longformer-base-4096-finetuned-squadv1 were not used when initializing LongformerForQuestionAnswering: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
      "- This IS expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "5it [43:13, 555.34s/it]Some weights of the model checkpoint at valhalla/longformer-base-4096-finetuned-squadv1 were not used when initializing LongformerForQuestionAnswering: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
      "- This IS expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "6it [53:18, 572.09s/it]Some weights of the model checkpoint at valhalla/longformer-base-4096-finetuned-squadv1 were not used when initializing LongformerForQuestionAnswering: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
      "- This IS expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "7it [1:03:54, 592.90s/it]Some weights of the model checkpoint at valhalla/longformer-base-4096-finetuned-squadv1 were not used when initializing LongformerForQuestionAnswering: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
      "- This IS expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "8it [1:19:16, 594.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# ans_path = \"D:/Machine Learning/scrapper and extraction/valhalla_answers\"\n",
    "final_ans = []\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\") # Equivalent to SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "for i, company in tqdm(enumerate(company_names)):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\")\n",
    "    Model = AutoModelForQuestionAnswering.from_pretrained(\"valhalla/longformer-base-4096-finetuned-squadv1\", return_dict=False)\n",
    "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    directory = f\"D:/Machine Learning/scrapper and extraction/Valhalla/data/{company}\"\n",
    "    docs = load_documents(directory)\n",
    "    docs = split_documents(docs)\n",
    "    ans = []\n",
    "\n",
    "    for ques in questions:\n",
    "        ques = ques\n",
    "        text = get_relevant_docs(docs, ques, embeddings)\n",
    "        encoding = tokenizer(ques, text, return_tensors=\"pt\")\n",
    "        input_ids = encoding[\"input_ids\"]\n",
    "        # default is local attention everywhere\n",
    "        # the forward method will automatically set global attention on question tokens\n",
    "        attention_mask = encoding[\"attention_mask\"]\n",
    "\n",
    "        start_scores, end_scores = Model(input_ids, attention_mask=attention_mask)\n",
    "        all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
    "\n",
    "        answer_tokens = all_tokens[torch.argmax(start_scores) :torch.argmax(end_scores)+1]\n",
    "        answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n",
    "\n",
    "        ans.append(answer)\n",
    "    \n",
    "    final_ans.append(ans)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Woebot Health', ' 7th annual', 'LG NOVA Mission for the Future ProgramThe LG Mission for the Future is a global engagement program for startups and entrepreneurs sponsored by LG NOVA – LG’s new innovation center', '', ' outside', ' LG NOVA', ' startups and entrepreneurs', ' third party beneficiary', ' 100', '', '<s>what is the growth rate of this company?</s></s>This is an opportunity to streamline innovation efforts into a sustainable and proactive process that incorporates leading employer insights to identify and prioritize emerging solutions for partnerships.MedTech Breakthrough AwardWoebot Health', '', ' Woebot', '<s>', ' free', '<s>total marketing budget?</s></s>This meeting serves as a chance for you to connect with the visionary leaders of our organization and gain a deeper understanding of our company’s values, long-term goals, and culture. It’s also an opportunity for the founders to get to know you better and assess how you align with our company’s vision. This will be in person or a zoom.Terms of Service | BAA | Privacy Policy | CCPA | SubprocessorsCopyright © NeuroFlow. 2022. All Rights Reserved\\n\\nThis form and direct email are not HIPAA-compliant, secure\\n\\nmethods of communication', '', '', ' 4,000', ' not HIPAA-compliant, secure\\n\\nmethods of communication', ' Digital Health Steering Committee, which through a range of workstreams is looking to transform regulatory software pathways by working collaboratively across industry, patient advocates and with regulators', '', ' LG Mission for the Future', '']\n",
      "24\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(final_ans[1])\n",
    "print(len(final_ans[4]))\n",
    "print(final_ans[1]==final_ans[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, company in enumerate(company_names):\n",
    "#     file = f\"D:/Machine Learning/scrapper and extraction/valhalla_answers/{company}.txt\"\n",
    "#     with open(file, \"w\") as f:\n",
    "#         for k, ques in enumerate(questions):\n",
    "#             f.write(f'Questions: {ques}\\nAnswer: {[ans[k] for ans in final_answer[i]]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'D:\\Machine Learning\\scrapper and extraction\\Valhalla\\answers'\n",
    "\n",
    "os.mkdir(path)\n",
    "\n",
    "for i, company in enumerate(company_names):\n",
    "    file = f\"{path}/{company}.csv\"\n",
    "    with open(file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"Questions\", \"Answers\"])\n",
    "        for k, ques in enumerate(questions):\n",
    "            writer.writerow([ques, final_ans[i][k]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
